# 深度学习（Deep Learning）

---

## ANN 引入

> 我们深度学习的任务其实和机器学习是一样的，就是传统的：
>
> 1. 回归
> 2. 分类
> 3. 生成式构式
>
> 我们接下来，要学习很多变种的起源（ANN，Artificial Neural Network）

## 最小二乘法拟合直线（逻辑回归）

> 目标：找到一条和所有点之间距离最小的直线
>
> 距离的衡量标准：$Distance = \hat{y} - y$，$\hat{y}$ 是点所在的高度，y 是我们计算出来的高度

<div style="display: flex; justify-content: center; background-color: white">
  <img src="https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg" alt="线性回归图">
</div>
<br>

### 数学建模

---

> 我们可以认为我们的 `直线` 函数是如下的公式：

$$
y = f(x_i) = \omega_ix_i + b
$$

- $y$ 预测值
- $x_i$ 实际特征
- $b$ 偏差
- $w_i$ 权重

> 用下面的公式来统计我们预测值和实际值的误差的误差函数：

$$
E(\omega, b) = \sum^{n}_{i = 1} \big(\hat{y} - f(x_i)\big)^2 = \sum^{n}_{i = 1}(\hat{y_i} - \omega x_i - b)^2
$$

- $\hat{y_i}$ 为每一个 $x_i$ 对应的实际值

> 你可以发现误差函数，是有两个未知量的，所以相当于是一个二元函数
>
> 而且这个二元函数，其实类似于 $y = x^2$，他其实就是这种形状：

![image-20250911161117596](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911161117596.png)

> [!important]
>
> 如果要是二元函数类比的话，就是这种形状的
>
> 那么这个二元函数一定存在极小值，我们可以对齐求一阶偏导 = 0，找极值点，直接算出未知量，没必要二阶偏导来验证了

![image-20250911161342117](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911161342117.png)

### 求解过程

> 偏导结果：

$$
\frac{\partial E}{\partial \omega} = -2 \sum_{i = 1}^n x_i \big(\hat{y_i} - \omega x_i - b\big) = 0 \\
\frac{\partial E}{\partial b} = -2 \sum_{i = 1}^n \big(\hat{y_i} - \omega x_i - b\big) = 0
$$

<br>

> 计算极小值，解上面两个方程，得到：

$$
\omega = \frac{\sum_{i = 1}^n (x_i - \bar{x})(\hat{y_i} - \bar{y})}{\sum_{i = 1}^n (x_i - \bar{x})^2} \\
b = \bar{y} -\omega \bar{x}
$$

其中：
$$
\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i, \quad \bar{y} = \frac{1}{n} \sum_{i = 1}^n \hat{y_i}
$$
<br>

## 梯度下降算法（Gradient Descent）

> [!warning]
>
> 在开始做这个梯度下降之前，我们先引入一些背景知识

<br>

### 计算一个神经元的过程

> [!note]
>
> 这是一个多个参数（这个参数数量不定）的输入、到输出、再到被激活 的全部过程，也就是 $y = f(x_1,\dots,x_n) \rightarrow Out = \sigma(y)$ 的全过程

![image-20250911134959508](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911134959508.png)

> 你看是不是挺像的，输入参数就是神经末梢那边，输出就是响应对应的电信号

![image-20250911133911078](C:\Users\shuhe\AppData\Roaming\Typora\typora-user-images\image-20250911133911078.png)

> 其中 $\Sigma$ 部分就是所有参数的加权平均值结果，bias 就是 + b 用于保持一个最低阈值，我们可以在下面介绍部分理解

<br>

### 偏差的作用（Bias）

---

#### 场景

> 神经元也不是啥情况都要响应电信号的，做一个举例：
>
> - 1、眼睛是否看到了老奶奶摔倒，是输入一
> - 2、听到了是否老奶奶在地上的呐喊，是输入二
>
> 假定你：`当且仅当两个事情都发生了，神经元才会传到去帮助老奶奶的电信号`

> [!note] 
>
> 我们再函数一点描述

- 函数有两个输入 $x_i ,i\in[1,2]$，每个输入代表上面假定的输入一 / 二：

  - $x_i = 1$：发生了
  - $x_i = 0.0$：没有发生

- 输出：如果输出信号 > 1 才会有帮助老奶奶的电信号。

  <img src="https://pwy.io/posts/learning-to-fly-pt1/assets/nn-3.svg" alt="img" style="zoom:500%;" />

> [!warning]
>
> 但是，只有一个事情发生的时候，我们就去找老奶奶。如果是搞错了咋整？
>
> 所以，我们预设定一个 bias 来做一个限制，我们是神经大条的人，就是 $b = -1$
>
> 我们是多虑的人，就 $b=1$ 

> 总之，你在此处，先理解为 `对电信号是否发出的增益或者减益` 吧，看到数学模型你会理解为什么这么假定的

<br>

####  数学模型

> 假设神经元输出公式是：

$$
y = x_1 + x_2 + b
$$

- $x_i$ = 0 或 1
- $b$ 是 bias（偏置）
- 暂不考虑权重 $\omega_i$

**假定我们是神经大条的人，即 $b = -1.0$**，所以输出变成：
$$
y = x_1 + x_2 - 1
$$

<img src="https://pwy.io/posts/learning-to-fly-pt1/assets/nn-4.svg" alt="img" style="zoom:500%;" />

#### 输入对应输出

|  x1  |  x2  | $y = x_1 + x_2 - 1$ |    说明    |
| :--: | :--: | :-------: | :--------: |
|  0   |  0   |     0     |   不激活   |
|  1   |  0   |     0     | 仍然不激活 |
|  1   |  1   |     1     |   激活   |

- bias = -1 让神经元“天然不激活（神经大条）”，一件事情的发生，并不会有激活的输出信号（去帮助老奶奶）
- 当只有两件事情都发生的情况 → 激活（l 两个是都发生才注意到有老奶奶摔倒了，得去帮下人家）

<br>

#### 总结

> [!important]
>
> 这个偏移就是激活的门槛，如果是 `b > 0` 就是激活更加容易，`b < 0` 就是激活特别困难
>
> 就是起到这么一个作用

<br>

### <span style="color:#FF3333;">神经网络（Neural Network）</span>

> 一个神经网络的计算结果，作为最后的结果，往往可信度是不够的，所以我们需要多个神经元网络来作为一层，然后他们接受输入，来计算出输出层所用的参数，也就是他是一个 `输入层的改进`

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911142743058.png" alt="image-20250911142743058" style="zoom:70%;" />

> 这些 Z 才会被用于最后的计算

#### 隐藏层计算公式

> [!note]
>
> 对于隐藏层里的某个神经元 $j$，它的输入来自前一层（比如输入层或前一个隐藏层）。

$$
h_j^{(m)} = f\Big(\sum_i^{n}  w_{ij}^{(m)} x_{i} + b_j\Big)
$$

- $i$ 是第几个用于计算的数值的索引
- $j$ 是要输出的第几个神经元的索引
- $m$ 是当前对应的第几个层
- $f()$ 是激活函数
- $x_i$ 可以是输入层的输入也可使隐藏层的输出
- $w_{ij}^{(m)}$ 第 i 个输入为了计算出第 j 个隐藏层神经元所用的权重（第 m 层）

<br>

#### 输出层计算公式

---

> 如果只有一个输出神经元：

$$
y = f\Big(\sum_i w_i h_i + b\Big)
$$

- 这里的 $i$ 来自最后一层隐藏层。

> 如果输出层有 $m$ 个神经元：

$$
\vec{y} = f\Big(\sum_j w_{ij} h_i + b_k\Big), \quad k = 1, 2, \dots, m
$$

- 每个输出神经元 $y_k$ 都有自己的一套权重 $w_{jk}$（对应前一层的所有神经元）。
- 每个输出神经元也有自己的偏置 $b_k$。
- 换句话说：**多输出就是在最后一层并列多个神经元，每个神经元各算一次“加权和 + 偏置 + 激活”**。

<br>

### 激活函数（Activation Function）

---

> 我们假定我们预测的结果是一个非线性的，如图：

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911124136517.png" alt="image-20250911124136517" style="zoom:73%;" />

#### 场景

> [!warning]
>
> 我们所预测的是一个非线性的曲线，那么对于我们的预测函数（ $y = \sum_{i=1}^{n}w_ix_i + b$ ）这种线性结构，在神经网络做叠加来说，函数图像永远都是一个直线，[这点可以看这个视频验证](https://www.bilibili.com/video/BV1Jy4y187Ez/?spm_id_from=333.337.search-card.all.click&vd_source=b47817c1aa0db593f452034d53d4273a)，另一个使用场景见 `反向传播`

![image-20250911133703317](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911133703317.png)

所以，我们需要一个激活函数，让其转换为一个非线性的结果

<br>

#### `Sigmod` 函数

> 左边是 `sigmod` 函数图像，**右边是其求导**（后面有用）

![image-20250911125342244](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911125342244.png)

- 你会发现，$output = \sigma(y)$ 我们 $y$ 的输出值，全部都被映射到了 $output\in(0,1)$ 的区间
- 而且这个函数本身不是直线的形状，这种 **非线性的曲线是可以拟合非线性的曲线的**

<br>

#### `tanh` 函数

![image-20250911165052173](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911165052173.png)

<br>

### 梯度下降

---

#### 背景

> 有了上面的理解，我们呢终于可以开始正题了
>
> 还记得，我们最小二乘法的时候，再求偏导 = 0 的时候嘛，计算如果直接计算这个值，其实是一件非常难得事情

$$
\frac{\partial E}{\partial \omega} = -2 \sum_{i = 1}^n x_i \big(\hat{y_i} - \omega x_i - b\big) = 0 \\
\frac{\partial E}{\partial b} = -2 \sum_{i = 1}^n \big(\hat{y_i} - \omega x_i - b\big) = 0
$$

> 这个最小二乘法也就只有一个权重，但是我们神经网络的函数起码是这样子的，暂不考虑 `激活函数和神经网络`

$$
y = w_1x_1 + w_2x_2 + \dots + w_nx_n + b
$$

那么我们的损失函数相当于：
$$
E(w_1\dots w_n, b)
$$


> [!warning]
>
> 这样子算偏导，然后求 = 0，计算机来做这个其实不太合适，更不要说，有些时候你哪怕知道有极小值，但是这个偏导数的线性方程也不一定解析解

<br>

#### 梯度下降的过程

> [!important]
>
> 那么问题来了，我们不求解方程，能不能像“摸黑下山”一样，每次沿着最陡的下坡方向走一点点，慢慢逼近山谷底部？
>
> 这就是梯度下降的直观想法：不要一次解出最优解，而是 `逐步往“最优解的方向”走`。

- 损失函数对参数的梯度告诉我们“往哪边走会变小”。于是参数更新公式是
- 这个偏导的目标就是让我们的 $\frac{\partial{E}}{\partial{参数}} \to 0$，就停止，因为 = 0，就是走到平坦的地方了，参数也几乎不会跟新了，因为 0 的存在，但是我们会提前停止，并不一定非得到 0
- 然后偏导数最接近 0 时候的 $\hat{w_{j}}、\hat{b}$，就是我们要求的值

> $$
> \hat{w_{j}} = w_j - \eta \frac{\partial E}{\partial w_j} \\
> \hat{b} = b - \eta \frac{\partial E}{\partial b}
> $$

- $\eta$ 就是 `学习率`，控制我们走的步子大小。
- $w_j$ 就是其中所有权重的第 `j` 个
- $\hat{w_{j}}、\hat{b}$ 为新的参数

> [!important] 
>
> 如果你的高数学的还不错的话，我们先 `假定损失函数都连续可导` 哈。应该还记得这么一个事，对一点的导数是可以判断局部区间内斜率的变化的，所以如果你的导数 > 0 那么局部区间是不是要递增了呢，这相当于说，我们正要往山的高不去爬了，反之就是往山下爬了。但是，`偏导数知识同你讲要往高出爬还是低处走，但是步子的大小，就是由学习率来定的了`，这个参数的的负面效果，也挺显而易见的，即：步子大了，直接就跳过山底，然后一直在山底上面震荡，步子小了，迭代的过程就超级慢。

总而言之就是：

 - 偏导为正 → 往正方向走会让损失变大。
 - 偏导为负 → 往正方向走会让损失变小。
 - $\eta$ 大 → 走快，但可能不稳定。
 - $\eta$ 小 → 稳定，但可能走很久。

<br>

### 批量梯度下降（Batch GD）

> [!note]
>
> 我们把所有的参数拿过来，拼成一个很大的向量：

$$
\vec{\theta} =
\begin{pmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n \\
b
\end{pmatrix}
$$

> `参数迭代更新公式` 其实就大差不大，但是方向导数要加权平均：

$$
g = \frac{1}{n+1}(\nabla_{\vec{\theta_{(old)}}}E)
\\
\vec{\theta_{(new)}} = \vec{\theta_{(old)}} + \eta \times g
$$

> 其中 $\nabla_{\theta}E$ 为：

$$
\nabla_{\vec{\theta}} E =
\begin{pmatrix}
\frac{\partial E}{\partial w_1} \\
\frac{\partial E}{\partial w_2} \\
\vdots \\
\frac{\partial E}{\partial w_n} \\
\frac{\partial E}{\partial b}
\end{pmatrix}
$$

<br>

### 正向传播（Forward Propagation）

> [!warning]
>
> 在神经网络里，`前向传播` 只负责 **计算输出和误差**，`更新参数` 必须靠 **反向传播 + 梯度下降** 反复迭代
>

![image-20250913044844106](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250913044844106.png)

> 正向传播的过程，[参考这个视频](https://www.bilibili.com/video/BV1AWPLe7EHo/?vd_source=b47817c1aa0db593f452034d53d4273a)，介绍下大致情况：
>
> - `sigmod` 作为激活函数
> - MSE 为误差评判标准
> - 最后出输出层也要用激活函数，仍然为 `sigmod` 函数

<br>

###  $\textcolor{pink}{反向传播(Back Propagation)}$

---

> [!note]
>
> 在这里你就知道为什么之前 `激活函数还得要求导` 了，代码是真真的不知道如何写
>
> 那么还记这个不，梯度下降的时候，必须要用这个<span style="color:#FFC0CB;">方向的偏导</span>作为更新用的参数

$$
\hat{w_{j}} = w_j - \eta \textcolor{pink}{\frac{\partial E}{\partial w_j}} \\
\hat{b_j} = b_j - \eta \textcolor{pink}{\frac{\partial E}{\partial b_j}}
$$

#### 数学建模

> [!important]
>
> 但是，我要如何拿到我们的偏导数呢？链式求导反向推导，就是一个很好的选择
>
> ![image-20250913044844106](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250913044844106.png)
>


> 有一个偏导关系是很好确定的 
> $$
> \textcolor{pink}{\frac{\partial E}{\partial out_{o1}}} =
> \frac{\partial MSE}{\partial out_{o1}} = 
> \frac{\partial}{\partial out_{oi}}
> \big [\frac{1}{2}\sum_{i = 1}^{2}(target_{o_{i}} - out_{o_{i}})^2\big] = 
> out_{o_{i}} - target_{o_{i}}
> $$
> 而且，由于输出值，我们在正向传播的时候是算出来了，然后实际值就是 target，所以这个在反向传播的时候，是一个定值，那么让我们看下，神经元如何拿到激活之前的状态呢？

$$
\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o1}}} = 
\textcolor{pink}{\frac{\partial E}{\partial out_{o1}}}
\times
\frac{\partial out_{o1}}{\partial net_{o1}}
$$

而这个 out 和 net 的偏导，正好就是激活函数的求导，且输入的参数就是 $out_{o1}$：

$$
\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o1}}} = 
(out_{o_{1}} - target_{o_{1}})
\times
\sigma(out_{o_1})\big(1-\sigma(out_{o_1})\big)
$$

> [!important]
>
> 进一步，我们就可以顺着链条拿到偏导数和误差的关系链了： 
> $$
> \textcolor{gold}{\frac{\partial E}{\partial w_5}} = 
> \textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_1}}}
> \times
> \frac{\partial net_{o_1}}{\partial w_5} =
> \big(\sigma(net_{o_1})[1-\sigma(net_{o_1})]\big) \times
> out_{h_1}
> 
> \\
> 
> \frac{\partial E}{\partial b_2} = 
> \textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_1}}}
> \times
> \frac{\partial net_{o_1}}{\partial b_2} = \sigma(net_{o_1})[1-\sigma(out_{o_1})]
> $$

继续推导节点，我们又可以再一次拿到关于权重的偏导，我们需要知道权重之前的偏导链：
$$
\textcolor{yellow}{\frac{\partial E}{\partial out_{h1}}} =

\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_1}}}
\times
\frac{\partial net_{o_1}}{\partial out_{h1}} + 

\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_2}}}
\times
\frac{\partial net_{o_2}}{\partial out_{h1}} = 

\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_1}}}\times w_5 +
\textcolor{lightgreen}{\frac{\partial E}{\partial net_{o_2}}}\times w_7

\\

\textcolor{cyan}{\frac{\partial E}{\partial net_{h1}}} = 
\textcolor{yellow}{\frac{\partial E}{\partial out_{h1}}}
\times
\frac{\partial out_{h1}}{\partial net_{h1}} = 
\textcolor{yellow}{\frac{\partial E}{\partial out_{h1}}}
\times \sigma(out_{h_1})[1-\sigma(out_{h_1})]
$$

> [!important]
>
> 在这里就拿到了和偏导的关系了
> $$
> \textcolor{gold}{\frac{\partial E}{\partial w1}} = 
> \textcolor{cyan}{\frac{\partial E}{\partial net_{h1}}}
> \times
> \frac{\partial net_{h1}}{\partial w_1} 
> = 
> \textcolor{cyan}{\frac{\partial E}{\partial net_{h1}}}
> \times
> x_1
> 
> \\
> 
> \textcolor{gold}{\frac{\partial E}{\partial b_1}} = 
> \textcolor{cyan}{\frac{\partial E}{\partial net_{h_1}}}
> \times
> \frac{\partial net_{h_1}}{\partial b_1} 
> = 
> \textcolor{cyan}{\frac{\partial E}{\partial net_{h_1}}} \times 1
> $$



<br>

#### 代码实现思路

> 在我们计算权重和偏导的关系之前，我们总是需要先拿到未激活的偏导链，未激活的偏导链来自于上一层的传播的偏导，你会发现总是 $out$，所以你要清楚：
>
> - 层之间的传播应该是： $\frac{\partial E}{\partial out}$
> - 当前层要做的应该是转换：$\frac{\partial E}{\partial out} \to \frac{\partial E}{\partial net}$，需要用激活函数
> - 利用转换的 $\frac{\partial E}{\partial net}$ 去求 $\frac{\partial E}{\partial w_i}$ 来更新参数，需要用到
> - $\frac{\partial E}{\partial net}$ 计算出下一层的 $\frac{\partial E}{\partial out}$
>

**但是你会发现，每一次计算都是一条链的计算，为了一次都够计算多个，我们采用矩阵来加速运算**，你去看看这个文件手算一次的过程，就会发现，其实就是把上面我的描述写成了矩阵形式，虽然有多条线路，但是因为矩阵的的作用，变成了相当于是一个神经元

<br>

#### 激活函数和反向传播的关系

> 由于我们这里对输出没有用激活函数激活
>
> 假定激活是 `sigmod 函数`：

$$
\sigma(y') = \frac{1}{1-e^{-y'}}
$$

> 我这个链式表达可能并不严谨，但是大体思路是这样的
>
> 通过链式关系可知：

$$
\frac{\partial E}{\partial \omega_1} = \frac{\partial E}{\partial{\sigma}} \times \frac{\partial{\sigma}}{\partial{y'}} \times 
\frac{\partial{y'}}{\partial{h_1}} \times
\frac{\partial{h_1}}{\partial{\omega_1}} 

\\

\frac{\partial E}{\partial \omega_5} = 
\frac{\partial E}{\partial \sigma} \times
\frac{\partial \sigma}{\partial y’} \times
\frac{\partial y’}{\partial \omega_5}
 = (y'-y)\times\sigma(1-\sigma)\times h_1
$$

> 对 `sigmod 函数` 求导可得：

$$
\frac{\partial{\sigma}}{\partial{y'}} = \sigma(1-\sigma)
$$

<br>

#### 激活函数对于反向传播的利害

> `Sigmod 函数` 图像为：

![image-20250911125342244](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911125342244.png)

##### 收敛变慢

> - `正向传播` 存在幂运算，计算复杂度大。
> - `反向传播` 拿到值其输出不是以 0 为中心，导致网络在训练过程中的数据发生偏移使得收敛速度减慢。
>
> 因为我们函数的均值，也就是期望并不是 0 的嘛，所以总体会偏向于均值一点，那么得到的结果总是会有偏差

$$
\frac{\partial E}{\partial \omega_1} = \textcolor{cyan}{\frac{\partial E}{\partial{\sigma}}} \times
\sigma(1-\sigma) \times
w_5 \times
x_1
\\
\frac{\partial E}{\partial \omega_2} = \textcolor{cyan}{\frac{\partial E}{\partial{\sigma}}} \times
\sigma(1-\sigma) \times
w_6 \times
x_2
$$

##### 梯度消失

> 如果正向传播用了 `sigmod 函数`，那么输出的值就会落在 `(0,1)` 之间，但是其对应的导函数的值至少为原来的 `1/4`

![image-20250911210149851](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911210149851.png)
$$
\frac{\partial E}{\partial \omega_5} = 
\frac{\partial E}{\partial{\sigma}} \times
\textcolor{pink}{\sigma(1-\sigma)} \times
h_1
 
\\
\frac{\partial E}{\partial \omega_1} =
\frac{\partial E}{\partial{\sigma}} \times
\textcolor{pink}{\sigma(1-\sigma)} \times
w_5 \times
x_1
$$

> 这还只是一层，再多来几层，前面更新后的梯度几乎是 0 了，这就是 `梯度消失`

#### `ReLu 函数` 的好处

> $ReLu(y) = \max(y,0)$

![image-20250911165207342](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911165207342.png)

##### **收敛变慢**

- **Sigmoid 的问题**：在反传时，所有梯度都会乘上 $\sigma(1-\sigma)$，这个值始终在 $(0, 0.25)$ 之间。
   结果就是：无论你的输入多大，**梯度都被压缩**，而且大家都被压到同一个范围里，容易出现“绑在一起往同一个方向更新”的情况 → 收敛慢。
- **`ReLU` 的改进**：在 $x > 0$ 区间，导数恒为 **1**，梯度就能“原汁原味”地往前传播，不会被额外缩小。
   这意味着不同权重的更新率差异能更清晰地保留下来 → 收敛更快。

<br>

##### **梯度消失**

- **Sigmoid 的问题**：随着层数加深，每一层都会引入一个 $\sigma(1-\sigma)$ 因子，最大也只有 0.25。多个相乘以后，前面层的梯度会迅速趋近于 0，这就是所谓的 **梯度消失**。
- **`ReLU` 的改进**：在正区间，梯度恒为 1，反传时不会引入额外的缩小因子。即使几十层，也能保持较强的梯度 → 避免了梯度消失问题。

<br>

##### `ReLU` 的新问题

>  当然 `ReLU` 不是完美的：

- 在 $x \leq 0$ 时，导数是 0，这会导致一些神经元 **永久失活**（Dead Neurons），训练中永远不更新。
- 为了解决这个，有一些变种，比如 **Leaky ReLU**、**ELU**、**GELU** ，[可以在这里了解一下](https://www.bilibili.com/video/BV1qB4y1e7GJ?spm_id_from=333.788.videopod.sections&vd_source=b47817c1aa0db593f452034d53d4273a)

<br>

### 梯度下降算法停止条件

---

####  梯度足够小

当梯度向量的范数（大小）小于某个阈值时就停：
$$
\|\nabla_{\theta} E\| < \epsilon
$$
意思是：再往下走，方向几乎是水平的了，更新幅度也很小，继续算意义不大。

<br>

####  损失下降停滞

如果损失函数 $E(\theta)$ 在连续几次迭代之间的下降幅度小于阈值：
$$
|E^{(t)} - E^{(t-1)}| < \epsilon
$$
说明已经接近“谷底”，继续训练也没什么进展。

<br>

#### 参数更新量很小

如果参数更新几乎不变：
$$
\|\theta^{(t)} - \theta^{(t-1)}\| < \epsilon
$$
说明学习率和梯度的乘积已经趋近于零。

<br>

#### 实际工程里的常见做法

- **迭代次数 / epoch 数限制**：比如最多训练 100 个 epoch，不管结果如何都停。
- **验证集早停（Early Stopping）**：如果验证集损失不再下降，甚至开始上升（过拟合），就停止。
- **综合使用**：一般会同时设最大迭代次数和容忍阈值，双保险。

<br>

## 改进梯度下降算法

---

### 随机梯度下降算法（Mini Batch GD）

> 就是，我们并不更新所有的权重的值了，我们就随机抽一部分的权重，然后更新部分的权重，减少运算量来加快运算

![image-20250911215643002](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250911215643002.png)

> 我们知道，我们的算法存在最大的问题就是可能会再山谷来回震荡，这里我们就介绍一下改良算法

### 动量梯度下降算法（Momentum GD）

> [!warning]
>
> 老的算法：

$$
\vec{\theta_{(new)}} = \vec{\theta_{(old)}} + \eta \times g(\theta_{t-1})
$$

> 我们把新老关系换一下描述：

$$
\theta_t = \theta_{t-1} - \eta \, g(\theta_{t-1})
$$



每次更新只依赖 **当前位置的梯度**。没有历史记忆。

> 物理直觉：小球在斜坡上滚动时，不仅有当前位置的斜率信息，还有“惯性”。
>
> 我们引入一个速度项 $v$：

$$
v_t = \gamma \, v_{t-1} + \eta \, g(\theta_{t-1}) \\
\theta_t = \theta_{t-1} + v_t
$$

- $v_t$：当前“速度”，就是累计的梯度方向
- $\gamma \in [0,1)$：动量系数，常见值 $0.9$，控制保留多少“过去的惯性”，有点阻尼的感觉
- $\eta$：学习率
- 初始时，$\theta_{t} = v_t$

> [!note]
>
> 惯性是因为：我们每一次的计算都会存入上一次的速度，所以如果上一次速度 > 0，这一次斜率 > 0。就进一步加速，反之就加速的慢点。

<br>

### 学习率的优化（Warm Up）

> 导致震荡最大的原因就是步子的长度，一开始步子迈的大是好事，但是后面的话步子大，就在山谷两边来回踱步，就是跨不到中间部分，`所以我们希望学习率也可以自适应的`

#### `AdaGrad` 算法

> `核心想法`：**为每个参数设置一个单独的学习率**，并且这个学习率会随着训练动态缩小，为此我们引入一个 $\gamma$
> $$
> \gamma \longleftarrow \gamma + g(\theta_{t-1})^2
> $$
> `公式`：
> $$
> \theta_{t} = \theta_{t-1} - \frac{\eta}{\sqrt{\gamma}+ \epsilon}  g(\theta_{t-1})
> $$

- $G_t = \sum_{i=1}^t g_i^2$，是到目前为止每个参数的“累计平方梯度”
- $\epsilon$ 是一个很小的数，防止除零
- $g = \frac{1}{n}\sum\nabla_{\theta} E$

<br>

> [!important]
>
> 我们讲一下实现的思路，我们的学习率就是除以了一个梯度历史以来的方差
>
> 在前期的时候，梯度变动平方和是很小的，所以我们除以的底数是很小的，于是学习率很大，变动的非常剧烈，
>
> 但是到了后面之后，随着不变的累计，底数就越来越大，于是学习率就变得很小，然后变动的就不那么剧烈了。

<br>

#### `RMSProp` 算法

> [!warning]
>
> `AdaGrad` 有一个很大的问题：`累积平方梯度无限增大导致学习率过小`
>
> 这个还是很显然意见的，如果增大的特别块，马上学习率小了，到后面累计的特别小，增量变化也很小，步子小了，再找到极小值之前，估计迭代次数都达到预设值的早停值了。

##### 累积梯度平方（指数衰减）

> 我们给两个值动态的加权平均就好了，一个增大，另一项值就会减小，两个有一个动态平衡的关系。
>
> 但是比较可惜的是，这个 $\rho$ 是我们手动调节的，就很违背初心，我们当初就是为了要一个自动的调控，现在又需要手动了

$$
\gamma \longleftarrow \rho\,\gamma + (1-\rho)\, g(\theta_{t-1})^2
$$

#####  参数更新

$$
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\gamma + \epsilon}} \, g(\theta_{t-1})
$$

- 每个参数的步长都自适应，梯度大的参数步长变小，梯度小的参数步长保持大
- $\epsilon$ 防止除零

<br>

### `Adam` 算法

> 目前都还在用的算法，主要做两件事：
>
> 1. **动量（Momentum）**
> 2. **自适应学习率**
>
> 他两个都做到了

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250912021028184.png" alt="image-20250912021028184" style="zoom:67%;" />

- $s$ 动量
- $r$ 学习率
- $\rho_1$ 惯性系数
- $\rho_2$ 平方梯度平滑系数
- $g = \frac{1}{n}\sum\nabla_{\theta} E$

> [!warning]
>
>  $\rho^t$ 这个 t 表示`自乘了 t 次`哈，是一个幂函数

> 看起来就是，`MPSProp` 那种动态调节给到了动量和学习率上面，用于控制动量和速度，引入的两个 $\rho$ 可不需要手动控制。
>
> 他们一开始只需要给一个比较大的  $\rho_i < 1$ ，后面会随着自乘越来越小，然后保持动量和学习率带来的影响稳定

<br>

# 卷积神经网络（CNN）

---

> [!note]
>
> 其实本质都是要丢到神经网络，但是为什么会有这个别类呢？因为我们要获取输入模型中的值，就得提取特征，而这种图像通过`卷积核`来提取图像特征，就是`CNN` 最大的特点

![image-20250916040038914](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916040038914.png)

<br>

## 通道

---

> - **灰度图像**：每个像素只有一个值 → 输入通道 = 1
> - **彩色图像（RGB）**：每个像素有 3 个值 → 输入通道 = 3
>
> 所以在 MNIST（灰度 28×28）里，第一层卷积的输入通道通常是 1。

<br>

## 卷积核（Convolutions）

> `卷积核`，就是特征提取器，一般都是 3x3 大小
>
> 被提取出来的结果数量，我们一般叫做输出通道，几个输出通道，就会有多少张特征图，也就是会有更多的特征提取器，来提取更多的特征图

![image-20250916040923984](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916040923984.png)

<br>

### 卷积核的运算规则

![image-20250916041019843](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916041019843.png)

![image-20250916041055764](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916041055764.png)

### 响应值（response）

> 在卷积核输出的“结果图”里，每一个数字并不是“像素灰度”本身，而是当前位置被卷积核“匹配”到的程度——学术叫**响应值**。
>
> - 数字越大 → 该位置与卷积核的模板越“像”；
> - 数字越小（负值越小）→ 越“反像”；
> - 接近 0 → 完全不像。

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916041655125.png" alt="image-20250916041655125" style="zoom:80%;" />

### 填充（Padding）

> [!warning]
>
> 你会发现，水平卷积效果居然不好。那是因为被降低维度的问题，导致边缘的特征丢失了，所以为了解决这个问题，就有了 padding

![image-20250916041953904](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916041953904.png)

> 加了一圈 0 之后，效果贼拉拉好，因为我们的卷积的结果升维了，所以能够拿到的信息也就多了

<br>

## 最大池化（Max Pooling）

---

> [!important]
>
> 就是了再卷积之后进一步压缩图片，让关键信息更加显眼，我们对数据进行拆组的方法，获取组中的最大值，拿出来
>
> 这里我们以垂直卷积核结果为例，作为说明

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916042407298.png" alt="image-20250916042407298" style="zoom:67%;" />

> 池化结果如下：

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916042221611.png" alt="image-20250916042221611" style="zoom:67%;" />

### 扁平化处理（flatten）

> 我们把池化后的数据，扁平化处理，三个元素一组的行向量从矩阵中拿出来，拼成一个列向量条
>
> 第二个图是，水平特征卷积核最大池化结果

![image-20250916042726923](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916042726923.png)

<br>

### 神经网络的接入

> 再拿到最大池化结果的列向量条，我们就送入神经网络进行训练

![image-20250916043133865](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916043133865.png)

<br>

### 交叉熵（Cross-Entropy）

> [!important]
>
> 对于多分类问题，我们就会先把输出的值转成 `One-Hot` 向量，然后利用`交叉熵` 来作为损失函数

![image-20250916193953989](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250916193953989.png)

<br>
