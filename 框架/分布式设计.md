# 分布式 & 集群设计

---

> 这篇笔记大多数探讨的是设计理念，以及对应环境如何搭建，当然也不是都有，具体搭建可以用 AI

<br>

## 概念以及他们两个的不同

> 集群（Cluster）是多个节点组成的“整体”
>
> 分布式（Distributed）是多个节点“协同完成一件事”

| 特性         | 集群 Cluster                                            | 分布式 Distributed                                          |
| ------------ | ------------------------------------------------------- | ----------------------------------------------------------- |
| 目标         | 高可用、负载均衡                                        | 分工协作、横向扩展                                          |
| 架构         | 多个相同服务组成一个整体                                | 多个不同服务组成系统的不同部分                              |
| 是否角色相同 | ✅ 是（多实例）                                          | ❌ 各节点有职责划分                                          |
| 举例         | 创建多个 Redis，虽然配合不同，但是整体表现为 Redis 服务 | 用户服务 + 商品服务 + 支付服务，但是不同 VPS 只执行一项任务 |
| 通信         | 通常对外表现为一个整体                                  | 节点之间主动通信协作                                        |
| 常配合       | 负载均衡 + 热备 + 主从同步                              | RPC 通信 + 服务发现 + 分布式事务                            |

<br>

## 多级缓存设计

- 需要 `lua` 的简单了解

> 传统缓存结构，我们 Redis 的文章里面有说一个“缓存雪崩”的问题，正好这个多级缓存就是解决方案
>
> `缓存雪崩`是指在同一时段大量的缓存key同时失效或者Redis服务岩机，导致大量请求到达数据库，带来巨大压力。

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250823212212602.png" alt="image-20250823212212602" style="zoom:75%;" />

> 一个不错的多级缓存结构
>
> 进程部分的缓存，我们可以参考自己语言的缓存

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250823212533156.png" alt="image-20250823212533156" style="zoom:67%;" />

### OpenResty

> OpenResty = Nginx + Lua 模块集合

![image-20250823213546212](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250823213546212.png)

#### lua 做反向代理

---

```json
location /api/v1/xxx {
    content_by_lua_file lua_script/proxy.lua;
}
```

#### openResty 获取请求参数

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250825145405018.png" alt="image-20250825145405018" style="zoom:82%;" />

### Canal

---

> [!note]
>
> `Canal`就是把自己伪装成MySQL的一个slave节点，从而监听master的`binary log`变化。再把得到的变化信息通知给`Canal`的客户端，进而完成对其它数据库的同步。
>
> 就是`订阅 & 发布`的小模型，监听的是数据库类，canal 提供了很多语言的接口，都可以互相调用
>
> 我们监听拿到消息变动之后要做的操作如下：

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250825151300031.png" alt="image-20250825151300031" style="zoom:80%;" />

## 总结

![image-20250825151422228](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250825151422228.png)

<br>

# 集群搭建技术和常见的集群

---

- [k8s 入门](https://www.bilibili.com/video/BV1Se411r7vY/?spm_id_from=333.337.search-card.all.click&vd_source=b47817c1aa0db593f452034d53d4273a)

## k8s 技术

> 核心组件

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250807191250334.png" alt="image-20250807191250334" style="zoom:47%;" />

<br>

## Redis 集群

> [!warning]
>
> 为什么我们需要一个集群

![image-20250809155529696](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809155529696.png)

### Redis 持久化

> 主要有一下两种方式
>
> 1. RDB：全称 RedisDatabase Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所 有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。
> 2. AOF：全称为AppendOnlyFile（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文
>    件。

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809155848511.png" alt="image-20250809155848511" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809155935783.png" alt="image-20250809155935783" style="zoom:70%;" />

![image-20250809160911246](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809160911246.png)

<br>

### Redis 主从模式搭建

---

![image-20250821182117101](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821182117101.png)

> 先编写我们的 composer 文件

#### 环境搭建

```yaml
# 指定 Docker Compose 文件版本
version: "3.9"

# 定义服务（容器）
services:
  # 主 Redis 服务
  redis-master:
    image: redis:7-alpine  # 使用 Redis 7 的 Alpine Linux 镜像（轻量级）
    container_name: redis-master  # 容器命名为 redis-master
    command: ["redis-server", "--appendonly", "yes"]  # 启动 Redis 并开启持久化（AOF 模式）
    ports:
      - "7001:6379"  # 将宿主机的 7001 端口映射到容器的 6379 端口（Redis 默认端口）
    volumes:
      - master-data:/data  # 挂载名为 master-data 的卷到容器内的 /data 目录（用于持久化数据）
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]  # 健康检查：通过发送 PING 命令确认服务是否正常
      interval: 5s  # 每 5 秒检查一次
      timeout: 3s   # 超时时间为 3 秒
      retries: 10   # 重试 10 次后才标记为不健康
    networks:
      - redis-net  # 连接到 redis-net 网络

  # 第一个 Redis 副本（从节点）
  redis-replica-1:
    image: redis:7-alpine
    container_name: redis-replica-1
    command: ["redis-server", "--appendonly", "yes", "--replicaof", "redis-master", "6379"]  # 关键参数：指定主节点为 redis-master:6379
    depends_on:
      - redis-master  # 依赖主节点，确保主节点先启动
    ports:
      - "7002:6379"  # 宿主机 7002 端口映射到容器 6379 端口
    volumes:
      - replica1-data:/data  # 副本1的独立数据卷
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "ping"]  # 检查副本节点是否可访问
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - redis-net

  # 第二个 Redis 副本（从节点）
  redis-replica-2:
    image: redis:7-alpine
    container_name: redis-replica-2
    command: ["redis-server", "--appendonly", "yes", "--replicaof", "redis-master", "6379"]  # 同样指定主节点
    depends_on:
      - redis-master
    ports:
      - "7003:6379"  # 宿主机 7003 端口映射到容器 6379 端口
    volumes:
      - replica2-data:/data  # 副本2的独立数据卷
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "6379", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      - redis-net

# 定义数据卷（用于持久化存储）
volumes:
  master-data:    # 主节点数据卷
  replica1-data:  # 副本1数据卷
  replica2-data:  # 副本2数据卷

# 定义网络
networks:
  redis-net:
    driver: bridge  # 使用桥接网络模式，使容器间可通过容器名通信
```

> ## 创建环境

```bash
# 拉起 composer 环境
docker compose up -d

# 查看复制状态
docker exec -it redis-master redis-cli info replication | grep -E 'role|connected_slaves|slave'
docker exec -it redis-replica-1 redis-cli info replication | grep role
docker exec -it redis-replica-2 redis-cli info replication | grep role
```

> 执行查看状态的第一条命令，你会拿到下面的信息

```shell
❯ docker exec -it redis-master redis-cli info replication | grep -E 'role|connected_slaves|slave'
# 输出
role:master
## 从机信息
connected_slaves:2
slave0:ip=172.20.0.3,port=6379,state=online,offset=140,lag=1
slave1:ip=172.20.0.4,port=6379,state=online,offset=140,lag=1
```

#### 数据同步

##### 全量同步

> [!note]
>
> 这个 repl_backlog 存储的就是我们主机做过的 redis 命令，然后 slave 用 offset 来计算和主机之间少做了多少的操作
>
> 这个 repl_backlog 是一个**环形缓冲区**（ring buffer），然后用于缓存局部差

![image-20250821173959878](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821173959878.png)

![image-20250821174150414](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821174150414.png)

##### 增量同步

> [!important]
>
> 不用担心 offset 溢出，他是类似与循环队列的结构，算的是差值，一般来说差值不会出现占满循环队列的情况，就是说，他们之间的记录数据总是同步差一些些的，真要有也就是两种情况：
>
> 1. 压根没同步过的第一次同步
> 2. 从机太久没同步了，差值过大
>
> 但是不论是那种情况，执行一次全局同步就好了

![image-20250821175141339](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821175141339.png)

### 哨兵模式

> 主从模式解决了从机宕机的问题，但是主机要是宕机了怎么办呢，这个就是靠我们的哨兵模式

<br>

#### 哨兵的作用

![image-20250821194843900](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821194843900.png)

##### 服务状态监控

> 要是我们指定数量的哨兵认为 master 不在线的话，那就切换其他可用的 slave 为 master

![image-20250821195104966](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821195104966.png)

#### 主从切换

![image-20250821195516155](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250821195516155.png)

<br>

#### 环境搭建

> 创建一主二从，加三个哨兵的模式
>
> > [!important]
> >
> > 目录结构建议：
>
> ```txt
> .
> ├── docker-compose.yml
> ├── sentinel1.conf
> ├── sentinel2.conf
> └── sentinel3.conf
> ```

```yaml
version: "3.9"

services:
  # ... 这个部分省略同主从模式

  redis-sentinel-1:
    image: redis:7-alpine
    container_name: redis-sentinel-1
    command: ["redis-sentinel", "/etc/redis/sentinel1.conf"]
    volumes:
      - ./sentinel.conf:/etc/redis/sentinel1.conf
    ports:
      - "26379:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  redis-sentinel-2:
    image: redis:7-alpine
    container_name: redis-sentinel-2
    command: ["redis-sentinel", "/etc/redis/sentinel2.conf"]
    volumes:
      - ./sentinel.conf:/etc/redis/sentinel2.conf
    ports:
      - "26380:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

  redis-sentinel-3:
    image: redis:7-alpine
    container_name: redis-sentinel-3
    command: ["redis-sentinel", "/etc/redis/sentinel3.conf"]
    volumes:
      - ./sentinel.conf:/etc/redis/sentinel3.conf
    ports:
      - "26381:26379"
    depends_on:
      - redis-master
    networks:
      - redis-net

volumes:
  master-data:
  replica1-data:
  replica2-data:

networks:
  redis-net:
    driver: bridge

```

> `sentinel.conf` 文件内容构成

```yaml
port 26379		# 哨兵端口
dir /tmp		# Sentinel 内部存储文件，不需要持久化
# 这里就是告诉 Sentinel 去盯住 redis-master:6379，而且要至少 2 个哨兵投票通过才进行切换
sentinel monitor mymaster redis-master 6379 2
# 如果在 5 秒内没得到响应，Sentinel 就会认为这个 master 可能挂了
sentinel down-after-milliseconds mymaster 5000
# 如果在 10 秒内还没完成主从切换，就认为这次 failover 失败，会再试
sentinel failover-timeout mymaster 10000
# 新 master 确定后，有几个从机可以同时进行全量同步
sentinel parallel-syncs mymaster 1
```

<br>

### 分片集群

> [!warning]
>
> 我们知道，主从和哨兵模式只解决了高可用、高并发读的问题。但是还有两个主要问题没有解决：
>
> - 海量数据存储的问题
> - 高并发写的问题

![image-20250823130440290](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250823130440290.png)

> [!note]
>
> 使用分片集群可以解决上述问题，分片集群特征：
>
> - 集群中有多个master，每个master保存不同数据
> - 每个master都可以有多个slave节点
> - master之间通过ping监测彼此健康状态
> - 客户端请求可以访问集群任意节点，最终都会被转发到正确节点

<br>

## Kafka 集群



<br>

# 分布式的一致性问题

---

> 学习前用的 sql 数据

```sql
-- 创建订单表
CREATE TABLE IF NOT EXISTS orders (
    order_id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    product_id INT NOT NULL,
    quantity INT NOT NULL,
    status VARCHAR(10) DEFAULT 'pending'  -- 订单状态：pending, success, failed
);

-- 创建账户表
CREATE TABLE IF NOT EXISTS accounts (
    account_id INT AUTO_INCREMENT PRIMARY KEY,
    user_id INT NOT NULL,
    balance DECIMAL(10, 2) NOT NULL DEFAULT 0.00  -- 用户余额
);

-- 创建库存表
CREATE TABLE IF NOT EXISTS inventory (
    product_id INT AUTO_INCREMENT PRIMARY KEY,
    product_name VARCHAR(100) NOT NULL,
    stock INT NOT NULL  -- 库存数量
);

-- 插入账户数据
INSERT INTO accounts (user_id, balance) VALUES
(1, 1000.00),
(2, 1500.00);

-- 插入库存数据
INSERT INTO inventory (product_id, product_name, stock) VALUES
(1, 'Product A', 100),
(2, 'Product B', 50);

-- 插入订单数据
INSERT INTO orders (user_id, product_id, quantity, status) VALUES
(1, 1, 2, 'pending'),
(2, 2, 1, 'pending');
```

## CAP定理

> CAP定理，也称为布鲁尔定理（Brewer's theorem），指出在分布式计算中，一个分布式系统不可能同时满足以下三个特性：

1. **一致性（Consistency）**：
   - 一致性意味着在分布式系统中，所有节点在同一时间看到的数据是相同的。即当一个写操作完成后，所有后续的读操作都应该返回这个最新的写入值。
2. **可用性（Availability）**：
   - 可用性意味着系统在任何时候都能响应用户的请求。即使部分节点发生故障，系统仍然能够继续提供服务。
3. **分区容错性（Partition tolerance）**：
   - 分区容错性是指系统能够继续运行，即使网络出现分区（即节点之间的通信出现故障）。分区容错性是分布式系统必须具备的特性，`因为在大规模分布式系统中，网络分区是不可避免的`。

> 根据CAP定理，一个分布式系统只能同时满足其中的两个特性，而不能同时满足全部三个特性。常见的三种组合是：

- **CA系统**：放弃分区容错性，适用于小规模系统。
- **CP系统**：放弃可用性，适用于需要强一致性的场景。
- **AP系统**：放弃一致性，适用于需要高可用性的场景。

## BASE理论

> BASE理论是对CAP定理中一致性和可用性权衡的进一步扩展，主要用于解决分布式系统中的`一致性问题`。BASE代表：

1. **基本可用（Basically Available）**：
   - 系统在出现故障时，仍然能够提供基本的服务，但可能只提供部分功能。
2. **软状态（Soft state）**：
   - 软状态指的是分布式系统中的数据可以存在临时的不一致状态，但最终会达到一致状态。这种状态是允许的，因为系统最终会通过某种机制（如异步复制）来达到一致性。
3. **最终一致性（Eventual consistency）**：
   - 最终一致性是指系统在经过一段时间后，所有节点的数据都会达到一致状态。这种一致性是通过异步复制和数据同步机制来实现的。

- BASE理论强调在分布式系统中，通过`牺牲强一致性`来提高系统的可用性和分区容错性。
- 最终一致性是BASE理论的核心，它允许系统在出现故障时仍然能够继续提供服务，并通过异步复制机制最终达到数据一致性。

![image-20250808184858916](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250808184858916.png)

## Seata 框架

---

> [!warning]
>
> 在微服务架构里，不同的服务通常会操作不同的数据库或资源（比如订单服务写订单库，库存服务减库存库，支付服务动资金库），这些操作如果放在传统单体里用本地事务很简单，但在分布式场景里就成了“如何保证大家一起成功或一起失败”的麻烦事。

> [go 也有 seata 框架](https://www.bilibili.com/video/BV1oz411e72T/?spm_id_from=333.337.search-card.all.click&vd_source=b47817c1aa0db593f452034d53d4273a)

![image-20250808193443112](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250808193443112.png)

![image-20250808193944451](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250808193944451.png)

> [!important]
>
> seata 框架对分布式事务的四种解决方式

![image-20250808194220790](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250808194220790.png)

### XA 模式

> [!note]
>
> 在准备阶段，事务管理器（Transaction Manager，TM）会向所有参与事务的资源管理器（Resource Manager，RM）发送准备请求（prepare request）。每个RM在本地执行事务操作，并将事务的结果（成功或失败）返回给TM。

![image-20250809115940590](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809115940590.png)

> - 锁定数据库这种资源所开销的时间过长，因为最终的提交是要等着所有相关的微服务都完成才可以执行
> - 所以在等待其他服务完成之前，自己也不允许别的业务在执行

![image-20250809120212953](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809120212953.png)

### AT 模式

> 我们知道 XA 就是因为需要等着别的 RM 的执行结果，自己要确定别的业务执行了才可以确保这条任务是确实需要实际执行的。
>
> 那么我们就不等了直接本地执行就好了，如果刚才那条业务别人出了问题，那么我们再回滚未执行的操作就好了。就和虚拟机快照一样的

![image-20250809121614453](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809121614453.png)

> [!warning]
>
> AT 模式的脏写问题：因为我们是直接提交然后做快照，所以资源锁的时间是很短的，但是如果面对高并发总是会有数据读写的问题。
>
> - 譬如下面的案例，我们本地提交之后没有锁了，然后别的线程这个进来改了数据，事务1失败，开始回滚了，然后把数据覆盖了 80，所以本就应该是 80 的数据，就变成了 100。如果是真的钱相关的问题，这个就很严重了

![image-20250809122106453](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809122106453.png)

> 解决方法也没有多高明，就是加一个全局锁，等待事务 1 的完成最后的提交或者回滚业务之后，才释放，别的事务才得以执行。
>
> 全局锁和DB锁不一样，DB锁的话，是限制数据库资源的访问，但是全局锁只是一个行锁，控制数据的范围是局部的，所以依然还是比较快的，如果修改数据的对象不是同一个的话

![image-20250809123827883](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809123827883.png)

> [!warning]
>
> 我个人极其不推荐这个模式。尤其是当出现`非 seata 管理的事务`对回滚时候的数据发生了修改之后，就这一点，我完全放弃这个模式，`安全 >  效率`
>
> 这里说一下，seata 是如何发现其他事务对我们回滚时候数据动手脚的原理：seata 会存储两个版本的回滚，一个是回滚之前的数据 - 100，和 sql 执行之后的数据 - 90。然后这个别的事务修改了这个 90 为 80，然后事务1开始回滚，拿到自己回滚前最后一次修改保存的值 90，发现和当前的 80 对不上，说明别的事务动手脚了。

![image-20250809125230114](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809125230114.png)

### TCC 模式

> 这个是预留的空间不需要加锁，直接对`预留的空间操作`
>
> 唯一的缺点就是接口写的听麻烦的，其余就是业务处理麻烦，毕竟有可能失败，失败的话要重建任务，有可能造成重复读写

![image-20250809125438567](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809125438567.png)

![image-20250809154306697](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809154306697.png)

### Saga 模式

> 就是直接操作数据，没有任何隔离性，也没有锁，有脏数据的风险

![image-20250809154531062](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809154531062.png)

### 总结

> 总体看下来，比较好的模式就是 XA 和 TCC，但是 TCC 比较费人，虽然性能啥的都不错，但是业务逻辑长，写的特别多
>
> XA 就很轻松了，但是这个性能，确实不咋地，能满足一般的分布式需求

![image-20250809154709960](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250809154709960.png)

<br>

# 分布式锁

---

> [!warning]
>
> 背景：为了负载均衡，我们使用了，nginx 创建了多个后端，然后用代理让请求均匀分配到两个后端上，突然发现同一个用户的两个请求都成功了，即便我们做过 ID 限制防止同一个 ID 多次请求
>
> 于是便有了这个分布式锁的存在

> [!note]
>
> 参考文章
>
> - [go 分布式锁技术攻略](https://mp.weixin.qq.com/s?__biz=MzkxMjQzMjA0OQ==&mid=2247484313&idx=1&sn=905342d3fc1cdaf845a62a140bd5247e&poc_token=HIttrGij_XfrvO-UaWrpErLKQ7-SOETaXqTLeHyQ)

<br>

## 锁的分类

- 主动轮询型：该模型类似于单机锁中的`主动轮询 + cas乐观锁`模型，取锁方会持续对分布式锁发出尝试获取动作，如果锁已被占用则会不断发起重试，直到取锁成功为止 
- watch回调型：在取锁方发现锁已被他人占用时，会创建 watcher 监视器订阅锁的释放事件，随后不再发起主动取锁的尝试；当锁被释放后，取锁方能通过之前创建的watcher感知到这一变化，然后再重新发 起取锁的尝试动作

<br>

## 主动轮询分布式锁

---

> 实现操作主要有两种方式，一种直接在 redis-cmd 的交互端中，使用 set 来设置 redis 锁。另一种就是借用 Lua 脚本来实现操作。

![image-20250825223228522](https://raw.githubusercontent.com/MTsocute/New_Image/main/img/image-20250825223228522.png)

> 同一个用户 ID 的两个请求可能落在 A 和 B 两个进程上，那么我们

```go
// 伪代码
ok := SETNX("lock:user:123", 1, EX 3s)
if !ok {
    // 已经有人持有这个用户的锁，说明并发请求，直接返回失败
    return
}
// 拿到锁，执行业务逻辑
```



<br>
